{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchtext, nltk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This data file has been ranked already, using a keyword multiplier system I used as a rudimentary start to the\n",
    "idea of ranking job listings. As such, we can expect that any neural network that tries to learn from this data\n",
    "will most likely relearn the particular words and their multipliers, and only tangentially learn other things.\n",
    "Thus, we need a method to allow for active learning from user input, ideally in the form of job listing pairs\n",
    "that are shown to the user for binary ranking, which then feeds the learning system somehow. Otherwise this\n",
    "network cannot learn anything that does not already appear in a curated dataset, which defeats the point of the\n",
    "neural network in the first place as a replacement for keyword multiplier ranking.\n",
    "'''\n",
    "\n",
    "Listings = pd.read_csv('Data/Listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean all fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_to_clean in ['Position','Company','Location','Salary','Summary']:\n",
    "    Listings[column_to_clean].replace('\\n',' ', regex=True,inplace=True) #Replace all newline char's with spaces\n",
    "    Listings[column_to_clean].fillna('', inplace=True) #Replaces all NaN's with blank strings\n",
    "    \n",
    "    # remove all unusual text characters in the text and turn everything lowercase to reduce dictionary size\n",
    "    cleaningfunction = lambda piece_of_text: re.sub(r'\\.(?=[^ \\W\\d])', '. ',piece_of_text[column_to_clean].lower())\n",
    "    Listings[column_to_clean] = Listings.apply(cleaningfunction,'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naively apply a ranking system to the job listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Listings['Rating'] = np.linspace(1,0,len(Listings['Rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a 2xN dataframe [All textual data as one string, Rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DescriptionAndRank = pd.DataFrame({'Description': Listings['Position'] + ' ' + Listings['Company'] + ' ' + Listings['Location'] + ' ' + Listings['Salary'] + ' ' + Listings['Summary'],'Rating': Listings['Rating']})\n",
    "\n",
    "#Save to CSV (did this to avoid having to figure out why the torchtext.data.TabularDataset() function wasn't working)\n",
    "DescriptionAndRank.to_csv('Data/DescriptionAndRank.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a torchtext format dataset and split it into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DescriptionField = torchtext.data.Field(sequential=True, #words have order, sequence matters\n",
    "                            include_lengths=True, #batching function tries to batch similar-length lines together\n",
    "                            \n",
    "                            # NLTK recognizes hyphenated bigrams and understands mis-spelled words,\n",
    "                            # it takes our long string of text and breaks it into individual words or \"tokens\",\n",
    "                            # fixing it along the way. The default would have been a split() function.\n",
    "                            tokenize=nltk.tokenize.word_tokenize, \n",
    "                            use_vocab=True, # we're going to use the GloVe vocabulary vectorizer to turn tokens into integers\n",
    "                            batch_first = True) #batch dimension comes first in the tensor\n",
    "\n",
    "RankField = torchtext.data.Field(sequential=False, \n",
    "                            tokenize=None,\n",
    "                            include_lengths=None,\n",
    "                            use_vocab=None,\n",
    "                            batch_first = True,\n",
    "                            \n",
    "                            #default is torch.long, fine for integer representations of words but not 0-1 ranking\n",
    "                            dtype=torch.float)\n",
    "\n",
    "Fields = [('Description', DescriptionField),('Rank', RankField)]\n",
    "\n",
    "\n",
    "dataset = torchtext.data.TabularDataset('Data/DescriptionAndRank.csv','CSV',skip_header=True,fields = Fields)\n",
    "trainset, validset = dataset.split() #automatically splits train/validation into 0.7/0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the words into a 50-dimensional format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The beauty of the GloVe vocabulary model is that it has been trained to \"group\" certain words with other words\n",
    "so that meaning is approximated in numerical format. We are using the 50-dimensional version, which means there\n",
    "are 50 dimensions in which one word can be \"similar\" to any other word. So for instance, if the word 'dog' were\n",
    "represented in its 34th dimension with a floating point of 0.893833, you would expect to find that the\n",
    "34th dimension of the word 'puppy' was close to that numerical value. In this way, the neural network can learn to\n",
    "approximate meaning, without having to define absurdly complex almost step-wise functions for randomly-assigned\n",
    "word vectors.\n",
    "'''\n",
    "DescriptionField.build_vocab(trainset, max_size = 30000, vectors='glove.6B.50d') #max 30,000 words/tokens\n",
    "print('Unique tokens in Description vocabulary: {}'.format(len(DescriptionField.vocab)))\n",
    "print(DescriptionField.vocab.itos[2:102]) #print the most popular 100 tokens (the first two are the \"unknown\" and the \"padding\" tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, stride_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        kernel_dimensions = zip(filter_sizes,stride_sizes)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim),\n",
    "                                              stride = (ss,embedding_dim))\n",
    "                                    for (fs,ss) in kernel_dimensions\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20 # counterintuitively, increasing this much more seems to make the performance suffer.\n",
    "                # I think that may be due to the addition of padding characters to 'even the batch up'?\n",
    "                # The bigger the batch size, the bigger the difference in job description lengths, the more padding\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (trainset, validset), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=lambda x: len(x.Description)) #sort by the length of the job description, that way we group \n",
    "                        # similar-length job descriptions together, avoiding having too much padding on the ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = len(DescriptionField.vocab)\n",
    "EMBEDDING_DIM = 50 # when we turned our tokens into vectors, this was the length of the vector\n",
    "N_FILTERS = 100 # how many features the convolutions learn\n",
    "FILTER_SIZES = [2,3,4,5,6,8,9,10] # originally this example only cared about 2,3,4 lengths, but our text examples are very large\n",
    "STRIDE_SIZES = [1,1,1,1,2,2,3,4] # without increasing the stride, larger kernels start to really slow things down\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = DescriptionField.vocab.stoi[DescriptionField.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, STRIDE_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = DescriptionField.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = DescriptionField.vocab.stoi[DescriptionField.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#I changed it to SmoothL1Loss because this is kind of a regression problem. learning rank, not classification\n",
    "criterion = nn.SmoothL1Loss() #BCEWithLogitsLoss() \n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \n",
    "    We are trying to do regression, but this is still nice to see at least whether it does a good job putting\n",
    "    the rankings in the upper or lower half of the 0-1 distribution. If it can do better than 0.5, a random coin\n",
    "    flip, we're on the right track.\n",
    "    \"\"\"\n",
    "    # round truths and predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    rounded_truth = torch.round(y)\n",
    "    correct = (rounded_preds == rounded_truth).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    pbar = notebook.tqdm(total=len(iterator),position=1,dynamic_ncols=True)\n",
    "    for i,batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.Description[0]).squeeze(1)\n",
    "        loss = criterion(predictions, batch.Rank)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.Rank)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        #print(loss.item(), acc.item())\n",
    "        pbar.update(1) #update tqdm progress bar\n",
    "        pbar.display('Iteration %d: Batch loss = %.6f , Batch accuracy = %.3f' % (i, loss.item(), acc.item()), pos=0)\n",
    "    pbar.close()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    pbar = notebook.tqdm(total=len(iterator),position=1,dynamic_ncols=True)\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i,batch in enumerate(iterator):\n",
    "\n",
    "            predictions = model(batch.Description[0]).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Rank)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.Rank)\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            pbar.update(1) #update tqdm progress bar\n",
    "            pbar.display('Iteration %d: Batch loss = %.6f , Batch accuracy = %.3f' % (i, loss.item(), acc.item()), pos=0)\n",
    "\n",
    "    pbar.close\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "    #    best_valid_loss = valid_loss\n",
    "    #    torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    torch.save(model.state_dict(), 'RankPrediction-model.pkl')\n",
    "    \n",
    "    print('Epoch: {} | Epoch Time: {}m {}s'.format(epoch+1,epoch_mins,epoch_secs))\n",
    "    print('\\tTrain Loss: {} | Train Acc: {}%'.format(train_loss,train_acc*100))\n",
    "    print('\\t Val. Loss: {} |  Val. Acc: {}%'.format(valid_loss,valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
